# video-diffusion-papers
This seminar will focus on the latest developments in the field of diffusion models, particularly video diffusion models. Topics will include aspects such as temporal and identity consistency, efficiency, and applications specifically in the realm of human avatars.

[Slides Template](https://docs.google.com/presentation/d/1ZuFepQjEWgJRPAHN6Ev0tEGTCUb1k2e6qBYnSDnmzDI/edit?usp=sharing)

Resources:
<br>
- [Diffusion models blogpost (more intuitive approach)](https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/)
- [Blog covering many subjects in Diffusion models - highly recommended](https://sander.ai/)
- [Tutorail on diffusion models (in-depth and detailed)](https://arxiv.org/pdf/2403.18103)
<br>
<br>

| Title | Paper / Resource | Year | Why is it interesting? |          Asignee          | Recording | Slides
|:---:|:---:|:---:|:---:|:-------------------------:|:---:|:---:|
|Introduction||| <details><summary>read why</summary>Review of key concepts from our previous seminar, followed by a brief overview the new seminar</details> |       Shira Bar On        |[zoom](https://us02web.zoom.us/rec/share/ZAulhoMpQaWp-BOc3wNwYMJ62ocejrQBLoAjFkXt0ti4R5-cP_eMU3E4kNcB2Wb_.PLRAaW3YOh64uu9T)(^gr9RV*^)|[slides](https://docs.google.com/presentation/d/19J0melyX7DEN9HMQLEGHouVIdvGmNrHALqKDik-HgUQ/edit?usp=sharing) |
|Video Diffusion Model in Latent Space|[Video Probabilistic Diffusion Models in Projected Latent Space](https://arxiv.org/abs/2302.07685)<br>+<br>[Align Your Latents](https://arxiv.org/abs/2304.08818)|2023| <details><summary>read why</summary>Example of two different approaches for Video Latent Diffusion Models</details> |       Tal Ben Haim        |[zoom](https://us02web.zoom.us/rec/share/e7CgB-rYnz9qjN5Jroufs55lxJjSM_O8z42idmKUv31H8hdknNeRJ9bcLM_WU84Y.XNnpeFhdbVNCYY20)(mQe@N2^0)|[slides](https://docs.google.com/presentation/d/1asvEZyRZZ9gR_-aFu8EaYUma6EaPv2qwef9co9P7b18/edit?usp=sharing) |
|Denoising Diffusion Implicit Models (DDIM) and Distillation|[DDIM](https://arxiv.org/abs/2010.02502)<br>+<br>[Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed](https://arxiv.org/abs/2101.02388)|2020, 2021| <details><summary>read why</summary>DDIM: paper introducing deterministic sampling, distillation paper lowers the number of sampling steps to one by building on this concept</details> |       Tomer Stolik        |[zoom](https://us02web.zoom.us/rec/share/f_OXEOfKlq3BxRMNUfr6yNW56aL_Lf5aw68ouX2fPe_nImLkGnc3O5dvqrpEmsWQ.usYNx3kU1hqNF-Ly)(Dha$Ue7&)|[slides](https://docs.google.com/presentation/d/1ADU-N3EU3kzqu6X41c3z1agkBLL1bpEXb1k42EAh55M/edit?usp=sharing) |
|Diffusion Transformer|[DiT](https://arxiv.org/abs/2212.09748)|2022 (ICCV 2023)| <details><summary>read why</summary>Replacing the Unet base of the diffusion model with Transformer base, which improves results and the condition control</details> |Shira Bar On|[zoom](https://us02web.zoom.us/rec/share/ukUtCIlnqVFs5oVt315z8dj1RZil5BOCTKnagK6q7QMzo5D7T8oSLJuWwh9XvGPl.VmceGPixwvHey1Zu)(?g16iHvN)|[slides](https://docs.google.com/presentation/d/1ff8LmTMbj8Rg_nTD2-ZeN-vxvZh26FgfrjMsgv_ycMw/edit?usp=sharing) |
|AnimateDiff|[AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725)|ICLR'24 spotlight| <details><summary>read why</summary>Introducing the Idea of converting of-the shelf text-to-image diffusion model to text-to-video diffusion model</details> | Ganit Kupershmidt |[zoom](https://us02web.zoom.us/rec/share/1T0Nt3oGrrWq7JiPiP-8vRkLN0_Ijytf1rzeZfxclTgjmiZJ5iuvoePxv978hhnO.KqCGI1a9H9RfeYKl)(?yjK4Df0)|[slides](https://docs.google.com/presentation/d/1oKX_8ypD3dNfFYh7HzgLIu21wnIbKTjYMCyoXVEifZc/edit?usp=sharing) |
|Controlling Generative Video Models|Lightricks|| <details><summary>read why</summary>Generating realistic images and videos from text has marked a significant milestone. However, as human beings, our ambitions extend further. We aim not only for visually appealing outcomes but also for enhanced control on the generated outcome. In my talk, I will explore the techniques employed to precisely control model outputs to fulfill our specific requirements at Lightricks</details> |            Neomi Ken Korem            |[zoom](https://us02web.zoom.us/rec/share/c0ZZ_QlXDoWOfomfePhJns-ixBl8CLPY6nDAHwfRjUiKoDugwMjL5j3mitAVk27-.MIMVdFrLL2gPhwuI)(kG?j66Z2)|[slides](https://docs.google.com/presentation/d/11s9iNPhoUGHWWTEcZz0MmAhnYgRHl0awsarrCBWh5Og/edit) |
|Animate Anyone|[Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://arxiv.org/abs/2311.17117)|2023| <details><summary>read why</summary>Video diffusion for avatar animation, introducing ReferenceNet for identity preservation</details> |   Matan Feldman   |[zoom](https://us02web.zoom.us/rec/share/gU2MdGL3LOi3B3iZpQD5Jo7mYJqY6h0k_kr20V-1zJ0LXPQGfPRf5pviseWR1Sx_.h868bxglfXzafnlM)(mK!0A7?e)|[slides](https://docs.google.com/presentation/d/1xURe7q2XIZHn-oCqKk3VRVVUmAEN2EUTzucff8eDNLY/edit#slide=id.p) |
|EMO: Emote Portrait Alive|[EMO](https://arxiv.org/abs/2402.17485)|2024| <details><summary>read why</summary>Face Animation using video diffusion models, using one image and audio as inputs</details> |    Alon Mengi     |[zoom](link)(password)|[slides](https://docs.google.com/presentation/d/19_MFDicaCbEowY8MXkJnRUs8VsNi-fljzjv46keucuQ/edit?usp=sharing) |
|Gen1 (Runway) |[Gen1](https://arxiv.org/abs/2302.03011), [gen2 blog](https://research.runwayml.com/gen2)|2023| <details><summary>read why</summary>Gen 1, 2, and 3 models are among the known video diffusion models</details> |       Daniel Duenias        |[zoom](link)(password)|[slides](link) |
|Advanced Topics in Distillation|[Progressive Distillation](https://arxiv.org/abs/2202.00512)<br>+<br>[Consistancy Models](https://arxiv.org/abs/2303.01469)|2022, 2023| <details><summary>read why</summary>two approaches for lowering the number of sampling steps</details> |       [Amitay Nachmani](link)       |[zoom](link)(password)|[slides](link) |
|Rectified Flow |[Rectified Flow](https://openreview.net/pdf/910c5efa5739a5d2bef83d432da87d3096712ebe.pdf)<br>+<br>[InstaFlow](https://arxiv.org/abs/2309.06380)|2023| <details><summary>read why</summary>Rectified Flow optimizes diffusion models by straightening their transport paths, Instaflow applis this technique</details> |       Ofir Bar Tal       |[zoom](link)(password)|[slides](link) |
|Generative Image Dynamics|[Generative Image Dynamics](https://arxiv.org/abs/2309.07906)|CVPR 2024 Best Paper Award| <details><summary>read why</summary>Using diffusion models to generate image dynamics, using the Fourier domain</details> |   [Roy Hachnochi](https://github.com/roy-hachnochi)   |[zoom](link)(password)|[slides](link) |

